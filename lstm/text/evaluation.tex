\section{Evaluation} \label{sec:eval}
In this section, we evaluate the real-time RNN serving tasks on various platforms.
We start with the methodology of our experiments, followed by a discussion of performance and power comparisons across
these platforms.

\subsection{Methodology} \label{sec:methodology}
To evaluate RNN serving, we use the LSTM and GRU tasks from Baidu DeepBench as our benchmarks.
We evaluate the benchmarks across processor-based architectures including CPU and GPU, 
and spatial architectures including FPGA and CGRA.
Table \ref{tab:spec} shows the detailed specifications of the targeting hardware, 
which includes state-of-the-art high performance platforms in each of the commercialized categories.
Table \ref{tab:appconf} summarizes application configurations of each platform.

\paragraph{CPU} We implement the applications in TensorFlow 1.10, and evaluate our implementations on 
Intel Xeon Scalable Processor (Skylake) CPU.
We use the \texttt{LSTMBlockFusedCell} and \texttt{GRUBlockCell} kernels in TensorFlow.
We further enable AVX2 vector instructions for CPU evaluation. Due to lack of low-precision
support in both tool chain and platform, we use single-precision for our implementation.

\paragraph{GPU} We use TensorFlow with cuDNN Library to target NVIDIA Tesla V100 GPU from Google Cloud. 
cuDNN is a GPU-accelerator Library from NVIDIA that is specialized for deep learning.
We use 16-bit precision for our implementation on GPU.
On both CPU and GPU platforms, we run \emph{TensorFlow} profilers and collect the time spent 
only on evaluating the RNN cells.

\paragraph{Plasticine} We implement the applications in Spatial, which targets Plasticine.
Although Spatial has FPGA back-end support, Stratix 10 is not commercially available at the time of the submission of this work.
The current FPGA targets that Spatial support are not comparable to Stratix 10 both in terms of memory and compute capacity. 
Therefore, we only use Spatial to target Plasticine for this evaluation. However, our approach should generally benefit
an implementation on a high performance FPGA like Stratix 10.
We choose Plasticine configuration that matches the peak 8-bit FLOPS and
on-chip scratchpad capacity of a Stratix 10 FPGA. The exact configuration of Plasticine is shown in Table \ref{tab:conf}.
In order to minimize the overhead of low-precision support, Plasticine only supports 8-bit, 16-bit, and 32-bit element-wise 
operations, and mixed precision reduction operation. 
For our evaluation, the element-wise operations are performed in 8-bit precision, 
the first stage of the reduction is performed in 16-bit, 
while the remaining of the reduction and accumulation are performed in 32 bit operations.

To measure the performance, we use a cycle accurate simulator for Plasticine. 
We modified the simulator to model the proposed micro-architectural changes to support low-precision operations.
We use the area and power of individual CUs and network switches from the original Plasticine paper, 
and compute total area of configuration shown in Table \ref{tab:conf}. 
As discussed in Section \ref{sec:arch}, we reduce the number of stages in PCU from 6 stages to 4 stages with fused low-precision
operations and folded reduction tree. 
Low preicision function units can be used to compose full precision units. 
We conservatively estimate the area and power of PCU stays the same with our proposed change and reduced two stages. 
We also increase the PMU to PCU ratio to better match the compute to memory
ratio for RNN inference applications. To match the memory capacity of Stratix 10, we shrink the scratchpad capacity of 
each PMU from 256kB to 84kB.
For power calculations, we generate activity tracing of the CUs from simulation, and then integrate 
with characterized power of individual PCU to compute the total power. The power and area characterizations are based off
synthesis at 28nm technology at 1GHz clock frequency.

\paragraph{Brainwave} Finally, we also compared our results to Microsoft Brainwave framework.
For this evaluation, we compare to Brainwave implemented
on top of Intel Stratix 10 FPGA. Brainwave is synthesized at 250MHz and all operations are performed in
blocked low-precision floating-point format described in section~\ref{sec:blaslstm}.
\begin{table}[t]
\caption{Plasticine configuration.}
\label{tab:conf}
\centering
\scriptsize
\begin{tabular}{L{3cm}rL{2.5cm}r}
\toprule
\# Row                     & 24   & \# Column        & 24  \\
\# PCU                     & 192  & \# PMU           & 384 \\
\# Lanes in PCU            & 16   & \# Stages in PCU & 4   \\
Scrachpad capacity per PMU & 84kB &                  &     \\
\bottomrule
\end{tabular}
\end{table}

%% Stratix 10 2800 M20K MBits 229, MLAB 15 MBits, = 30.5MB
%% Source https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/hb/stratix-10/s10-overview.pdf
\begin{table}
\caption{Hardware specifications for target platforms.}
\label{tab:spec}
\scriptsize
\centering
\begin{tabular}{L{2.5cm}M{1.2cm}M{0.8cm}M{0.8cm}M{1cm}}
\toprule
  Specification        & Intel Xeon Skylake (Dual core) & Tesla V100 SXM2 & Stratix 10 280 FPGA & Plasticine\\
\midrule
Max Clock Rate (GHz) & 2.0/2.8*                  & 1.38/1.53*      & 1                   & 1 \\
On-chip memory** (MB) & 55                        & 20              & 30.5                & 31.5\\
Peak 32-bit TFLOPS   & --                      & 15.7            & 10                  & 12.5\\
Peak 8-bit TFLOPS    & --                        & --              & 48                  & 49\\
Technology ($nm$)    & 14                        & 12              & 14                  & 28\\
Die Area ($mm^2$)    & 64.4                      & 815             & 1200                & 494.37 \\
  TDP (W)    & 15                      & 300             & 148                & 160 \\
\bottomrule
\end{tabular}
* Base/Boosted Frequency
** Capacity of L3 cache for CPU, register file for GPU, and on-chip scratchpad for reconfigurable architectures.
  %* Computed with AVX512f instructions. \cite{markidis2018nvidia}
\end{table}

\begin{table}
\caption{Application configurations for target platforms.}
\label{tab:appconf}
\centering
\scriptsize
\begin{tabular}{L{1.8cm}M{1cm}M{1cm}M{1cm}M{1.5cm}}
\toprule
Platform                       & Intel Xeon Skylake & Tesla V100 SXM2 & Stratix 10 280 FPGA & Plasticine\\
\midrule
Software Framework             & TF+AVX2                   & TF+cuDNN        & Brainwave           & Spatial \\
Achieved Clock Frequency (GHz) & 2                         & 1.38            & 0.25                & 1 \\
Precision                      & f32                       & f16             & blocked precision   & mix f8+16+32\\
\bottomrule
\end{tabular}
\end{table}

\begin{table*}
\caption{Performance comparison of DeepBench Inference.}
\label{tab:eval}
\centering
\scriptsize

%%% =================== With Utilization  ========================
%\begin{tabular}{|L{0.6cm}|M{0.6cm}|M{0.6cm}|M{1.1cm}M{0.6cm}M{0.6cm}M{1.2cm}|M{1.1cm}M{0.6cm}M{0.6cm}M{1.2cm}|M{0.6cm}M{1.2cm}|M{1.2cm}|}
%\hline
  %\multicolumn{3}{|c|}{\sc Benchmarks}					&	\multicolumn{4}{c|}{\textsc{Latency} (ms)}							&	\multicolumn{4}{c|}{\sc Effective TFLOPS}							&	\multicolumn{2}{M{2cm}|}{\textsc{8-bit FLOPS Utilization} (\%)}			&	\sc Power (W)	\\ \hline
  %&	\sc H	&	\sc T	&	\sc Xeon Skylake	&	\sc Tesla V100	&	\sc BW &	\sc Plasticine	&	\sc Xeon Skylake	& Tesla V100	&	\sc BW	&	\sc Plasticine	&	\sc BW	&	\sc Plasticine	&	\sc Plasticine	\\ \hline
%%% =================== PASTE HERE ========================
%\multirow{5}{*}{\sc\bf LSTM}	&	256	&	150	&	15.75	&	1.69	&	0.425	&	0.042	&	0.010	&	0.09	&	0.4	&	3.8	&	0.8	&	7.7	&		\\
	%&	512	&	25	&	11.50	&	0.60	&	0.077	&	0.015	&	0.009	&	0.18	&	1.4	&	7.0	&	2.8	&	14.3	&		\\
	%&	1024	&	25	&	107.65	&	0.71	&	0.074	&	0.037	&	0.004	&	0.59	&	5.7	&	11.2	&	11.8	&	22.9	&		\\
	%&	1536	&	50	&	411.00	&	4.38	&	0.145	&	0.160	&	0.005	&	0.43	&	13.0	&	11.8	&	27.1	&	24.1	&		\\
	%&	2048	&	25	&	429.36	&	1.55	&	0.074	&	0.106	&	0.004	&	1.08	&	22.7	&	15.8	&	47.3	&	32.3	&		\\ \hline
%\multirow{6}{*}{\sc\bf GRU}	&	512	&	1	&	1.00	&	1.00	&	0.013	&	1.000	&	0.003	&	0.00	&	0.2	&	0.0	&	0.5	&	0.0	&		\\
	%&	1024	&	1500	&	449.00	&	33.77	&	3.792	&	1.720	&	0.042	&	0.56	&	5.0	&	11.0	&	10.4	&	22.4	&		\\
	%&	1536	&	375	&	2,730.00	&	13.12	&	0.951	&	0.910	&	0.004	&	0.81	&	11.2	&	11.7	&	23.3	&	23.8	&		\\
	%&	2048	&	375	&	5,040.00	&	17.70	&	0.954	&	1.580	&	0.004	&	1.07	&	19.8	&	12.0	&	41.2	&	24.4	&		\\
	%&	2560	&	375	&	7,590.00	&	23.57	&	0.993	&	2.460	&	0.004	&	1.25	&	29.7	&	12.0	&	61.9	&	24.5	&		\\
	%&	2816	&	750	&	25,850.00	&	55.48	&	1.987	&	6.430	&	0.003	&	1.29	&	35.9	&	11.1	&	74.9	&	22.7	&		\\
%%% =================== PASTE HERE ========================

%% =================== With Speedup and no power ========================
  \begin{tabular}{|L{0.6cm}|M{0.4cm}|M{0.4cm}|M{1.1cm}M{0.45cm}M{0.45cm}M{1.2cm}|M{1.1cm}M{0.45cm}M{0.45cm}M{1.2cm}|M{1.1cm}M{0.6cm}M{0.6cm}|M{1.2cm}|}
\hline
    \multicolumn{3}{|c|}{\sc Benchmarks}					&	\multicolumn{4}{c|}{\textsc{Latency} (ms)}							&	\multicolumn{4}{c|}{\sc Effective TFLOPS}							&	\multicolumn{3}{M{3cm}|}{\sc Plasticine Speedup (x)} & \sc Power (W)			\\ \hline
    &	\sc H	&	\sc T	&	\sc Xeon Skylake	&	\sc Tesla V100	&	\sc BW &	\sc Plasticine	&	\sc Xeon Skylake	& Tesla V100	&	\sc BW	&	\sc Plasticine	&	\sc Xeon Skylake	&	\sc Tesla V100	&	\sc BW	 & \sc Plasticine\\ \hline
%% =================== PASTE HERE ========================
\multirow{5}{*}{\sc\bf LSTM}	&	256	&	150	&	15.75	&	1.69	&	0.425	&	0.0419	&	0.010	&	0.09	&	0.37	&	3.8	&	376.3	&	40.4	&	10.2	&	28.5	\\
	&	512	&	25	&	11.50	&	0.60	&	0.077	&	0.0139	&	0.009	&	0.18	&	1.37	&	7.6	&	830.3	&	43.2	&	5.6	&	53.7	\\
	&	1024	&	25	&	107.65	&	0.71	&	0.074	&	0.0292	&	0.004	&	0.59	&	5.68	&	14.4	&	3,686.6	&	24.3	&	2.5	&	97.2	\\
	&	1536	&	50	&	411.00	&	4.38	&	0.145	&	0.1224	&	0.005	&	0.43	&	13.01	&	15.4	&	3,357.8	&	35.8	&	1.2	&	102.7	\\
	&	2048	&	25	&	429.36	&	1.55	&	0.074	&	0.1060	&	0.004	&	1.08	&	22.62	&	15.8	&	4,050.6	&	14.6	&	0.7	&	104.5	\\ \hline
\multirow{6}{*}{\sc\bf GRU}	&	512	&	1	&	0.91	&	0.39	&	0.013	&	0.0004	&	0.003	&	0.01	&	0.25	&	7.6	&	2,182.3	&	942.4	&	31.2	&	61.9	\\
  &	1024	&	1500	&	3,810.00	&	33.77	&	3.792	&	1.4430	&	0.005	&	0.56	&	4.98	&	13.1	&	2,640.3	&	23.4	&	2.6	&	109.1	\\
  &	1536	&	375	&	2,730.00	&	13.12	&	0.951	&	0.7463	&	0.004	&	0.81	&	11.17	&	14.2	&	3,658.3	&	17.6	&	1.3	&	114.6	\\
	&	2048	&	375	&	5,040.00	&	17.70	&	0.954	&	1.2833	&	0.004	&	1.07	&	19.79	&	14.7	&	3,927.5	&	13.8	&	0.7	&	101.2	\\
	&	2560	&	375	&	7,590.00	&	23.57	&	0.993	&	1.9733	&	0.004	&	1.25	&	29.69	&	15.0	&	3,846.4	&	11.9	&	0.5	&	117.2	\\ \hline
\multicolumn{3}{|c|}{\textsc{\bf Geometric Mean}}					&		&		&		&		&		&		&		&		&	2,529.3	&	29.8	&	2.0	&		\\
%% =================== PASTE HERE ========================
\hline
\end{tabular}
\end{table*}

\begin{table}
\caption{Loop unrolling and vectorization parameters for spatial architectures.}
\label{tab:param}
\centering
\scriptsize
\begin{tabular}{|L{0.5cm}|M{0.4cm}|M{0.4cm}|M{0.3cm}|M{0.3cm}|M{0.3cm}|M{0.3cm}|M{0.3cm}|M{0.3cm}|M{0.3cm}|}
\hline
  \multicolumn{3}{|c|}{\sc Benchmarks}						&\multicolumn{3}{c|}{\sc Stratix 9 BW} &						\multicolumn{4}{c|}{\sc Plasticine}							\\\hline
	&	\sc H	&	\sc T	&	$ru$	&	$hv$	&	$rv$	&	$hu$	&	$hv$	&	$ru$	&	$rv$	\\\hline
%% =================== PASTE HERE ========================
\multirow{5}{*}{\sc\bf LSTM}	&	256	&	150	&	\multirow{11}{*}{6}	&	\multirow{11}{*}{400}	&	\multirow{11}{*}{40}	&	6	&	\multirow{11}{*}{1}	&	4	&	\multirow{11}{*}{64}	\\ \cline{7-7} \cline{9-9}	\cline{2-3}
	&	512	&	25	&		&		&		&	\multirow{4}{*}{4}	&		&	\multirow{10}{*}{8}	&		\\	\cline{2-3}
	&	1024	&	25	&		&		&		&		&		&		&		\\	\cline{2-3}
	&	1536	&	50	&		&		&		&		&		&		&		\\	\cline{2-3}
	&	2048	&	25	&		&		&		&		&		&		&		\\ \cline{7-7}	\cline{2-3}
\multirow{6}{*}{\sc\bf GRU}	&	512	&	1	&		&		&		&	\multirow{6}{*}{2}	&		&		&		\\	\cline{2-3}
	&	1024	&	1500	&		&		&		&		&		&		&		\\	\cline{2-3}
	&	1536	&	375	&		&		&		&		&		&		&		\\	\cline{2-3}
	&	2048	&	375	&		&		&		&		&		&		&		\\	\cline{2-3}
	&	2560	&	375	&		&		&		&		&		&		&		\\	\cline{2-3}
	&	2816	&	750	&		&		&		&		&		&		&		\\\hline	\cline{2-3}
%% =================== PASTE HERE ========================
\end{tabular}
\end{table}

\subsection{RNN Performance Analysis} \label{sec:results}
Table \ref{tab:eval} shows the performance comparison of LSTM and GRU with various numbers of hidden units (H) and step sizes (T) over
the four platforms. Overall, both CPU and GPU significantly underutilize the available compute FLOPS.
In addition, they cannot meet the latency requirement for real-time serving for all problem sizes.
Both BW and Plasticine deliver promising latencies within 5ms for all problem sizes.
When serving very large RNNs, BW provides better performance
	with up to 2x better than Plasticine on the largest GRU (H=2816).
% Plasticine's 8-bit multiplier-accumulator (MAC) units are implemented using mixed precision multipliers and adders,
% 	which are more expensive than BW's MAC implementation.
% As a result, Plasticine embeds fewer number of MAC units while scaled at the same peak TFLOPS as BW.
% This explains the performance gap between Plasticine and BW while serving the large GRU.
When serving small and medium size RNNs, Plasticine performs better than BW
	with up to 30x better performance on small GRU (H=512).
We also observe that Plasticine delivers consistent FLOPS when serving all the problem sizes.

\paragraph{Processor-Based Architectures}
For CPU experiments, the RNN kernels from TensorFlow itself is not multi-threaded.
Since we focus on real-time serving of RNN applications, we use batch size of 1 for all of our benchmarks,
	which expose no parallelism outside the kernel level.
Consequently, the machine is still very underutilized even with AVX2 instruction.
Although one could implement RNN directly in c++,
	the MVM sizes in RNNs are too small to benefit from multi-threading due to the synchronization overhead.
V100 with cuDNN library provides significant acceleration compared to CPU.
	Nevertheless, the latency is still high.
This is because GPUs are designed for throughput oriented rather than latency sensitive workloads.
Provided that the library is based on BLAS3 routines, which are matrix-matrix operation, MVMs in 
RNN serving suffer from significant resource underutilization.
In Table \ref{tab:eval}, V100 shows very poor performance on GRU (H=512). This is likely due to
the initialization overhead which should not be timed.
From our evaluation, neither processor-based architectures are suitable for providing low-latency serving on
RNN applications.

\paragraph{Spatial Architectures} Table \ref{tab:param} shows the selected design parameters for each 
problem size for BW and Plasticine.
On Stratix 10, BW uses 6 tile engines ($ru$) with native dimension of 400 ($hv$) and 40 lanes ($rv$).
Large $hv$ and $rv$ improve the data-to-control ratio by amortizing the scheduling overhead over a large vectorized instruction.
However, this design choice aggravates the underutilization for small RNN feature sizes at 256 and 512.
Our implementation effectively uses $hv$ of size 1 by performing dot product instead of MVM, 
which prevents fragmentation in the $H$ dimension.
	With $hv=1$, all the intermediate buffers are stored in registers.
In contrast, BW uses register files of size $hv$.
In addition, our proposed implementation captures additional gate-level, X, and H parallelism as well as pipelining at element-wise functions.
In contrast, BW schedules these operations in time and dispatches corresponding instructions to drive the compute units.

A CGRA is less flexible than an FPGA when performing arbitrary low-precision operations. 
In this example,
	we increase memory density of Plasticine by supporting quantile precisions as described in Section \ref{sec:arch:varprec}.
All weights are stored in 8 bit format, so as the multiplication operations of MVM. 
The reduction and accumulation operations are implemented in mix of 16 and 32 bit precisions.
Hence, the peak FLOPS when performing mixed precision map-reduce is much less than the peak FLOPS for blocked low-precision format in BW.
As a result, Plasticine performs worse than BW on the large RNNs.

In addition, Plasticine delivers very consistent FLOPS for
different problem sizes. For small problem size, the dot product can be fully unrolled with $rv * ru$. Therefore, we can
increase $hu$ to explore additional parallelism across the hidden units. For large problem size, dot product becomes the bottleneck of
the pipeline. Hence, we reduce $hu$ and increase $ru$ to balance the throughput between dot product and element-wise operations.
In this example, BW uses a single set of parameters for all problem sizes.
Although one can potentially tune parameters for different problem sizes,
	doing so will incur re-synthesis and place-and-route on an FPGA,
	which is an order of magnitude longer than the compilation time needed for a CGRA design.
In addition, to exhaust hardware resources with a smaller $hv$,
	one would have to increase the number of matrix vector tile engines $hu\times ru$ in BW.
As a result, decoders and schedulers associated with these units
	will drive up the control-to-data overhead and deliver less FLOPS for larger problem sizes.

\subsection{Area and Power Analysis} \label{sec:results}
Table \ref{tab:spec} shows the die area comparison of different platforms.
	While the GPU has a publicly-reported die area measurement \cite{markidis2018nvidia},
	Xeon Skylake and Stratix 10 only have estimated die areas based on
	their estimated transistor counts \cite{inteldie}.
With the rough area estimates, we can see that while CPU has the smallest area in this case,
	the performance gap is too large even after we scale up to a 28-core server.
The GPU also delivers bad performance per area mostly due to the low utilization of compute FLOPS.
Stratix 10 delivers the best performance for the large RNNs,
	but with the largest die area estimates of 30 billion transistors \cite{stratix10die}.
Plasticine's die area is based on the synthesis results at 28nm,
	which is one generation older than all the other platforms.
With technology scaling,
	Plasticine should possess double the amount of compute and memory resources at 14nm for the same die area,
	which will roughly match Stratix 10's performance on all the RNN problem sizes.
At the same time, Plasticine is more than 2x smaller than Stratix 10,
	which could also contribute at least 2x - 60x performance per area improvement for all problem sizes.
Table \ref{tab:spec} shows the thermal design power (TDP) of the four platforms,
	which is the peak power achievable for any workloads \cite{inteltdp, stratix10tdp, v100spec}.
BW also reports a measured peak power for the given set of benchmarks of 125W.
Table \ref{tab:eval} shows the simulated power for Plasticine for each benchmark.
	Overall, the peak power among benchmarks for Plasticine is 118W,
	which is slightly less than the peak power compared to BW.
% With technology scaling, Plasticine can match the performance of Stratix 10
% on the large problem size with roughly the same power.
