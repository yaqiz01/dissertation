\prefacesection{Abstract (WIP)}

%The rise of ``dark silicon'' in the semiconductor industry due to the end of Dennard scaling 
%started the multicore era in processor design since 2005.
%Nonetheless, the performance scaling in multicores is soon coming to a limit, which motivates a new
%class of spatial architecture--reconfigurable dataflow accelerators (RDAs)--that delivers high throughput 
%and energy efficiency in computation to keep up with the performance demand.
%To adapt to the compute intensity in modern data-analytic workloads, particularly in deep learning, RDAs are increasing to a scale
%that was unprecedented before.
%The increase in scale introduces new challenges in on-chip network design to maintain the energy efficiency and scalability of RDAs.
%Furthermore, managing and using RDAs at this scale also require new strategies in mapping, memory management, and flexible control
%to saturate compute throughput of the accelerator. 

%In this talk, I will talk about architectural design and compilation techinques that improves the scaling efficiency of a RDA developed 
%at Stanford--Plasticine.
%Staring with a static-dynamic hybrid network, we show that the hybrid network can improves energy efficiency while providing
%guaranteed success in placement and routing.
%Next, I will talk about compiler techinques that convert applications' complex control hierarchies, such as nested loops and branch conditions, into a streaming dataflow representation that can be efficiently executed by Plasticine with distributed on-chip resources.
%The compiler implements (a) a peer-to-peer (p2p) control paradigm inferred from an imperative programming style that minimizes synchronization overhead, and (b) a mapping strategy that decomposes the computation and memory in a program across a distributed heterogeneous resources.
%By applying these techniques, we show that Plasticine is able to outperform state of art accelerators, such as GPUs and FPGAs, in
%both performance and performance/Watt in various dense, sparse, and streaming applications.

With the slowdown of Mooreâ€™s Law, specialized hardware accelerators are gaining tractions 
for delivering 100-1000x performance improvement over general-purpose processors. 
As performance scaling in multicores coming to a limit~\cite{multicorescale}, a new
class of accelerators--reconfigurable dataflow architectures (RDAs)--are particularly promising in 
offering high-throughput and energy-efficient acceleration that keeps up with the performance demand.
Instead of dynamically fetching instructions like in traditional processors, RDAs have flexible datapath 
that can be statically configured to parallelize and pipeline the program spatially across
the on-chip resources. 
The pipelined execution model and explicitly-managed scratchpad in RDAs eliminate
the performance, area, and energy overhead in dynamic scheduling and conventional memory hierarchy.

To adapt to the compute intensity in modern data-analytic workloads, particularly in deep learning, 
RDAs are increasing to a scale that was unprecedented before, compared to classic coarse-grained
reconfigurable architectures (CGRAs).
Plasticine is an example of a hierarchical RDA with coarse-grained reconfigurable tiles and a
streaming interconnect.
Prior work has shown an up to 76x performance/watt benefit from Plasticine over a Stradix V FPGA 
due to advantage in clock frequency and resource density.
The increase in scale introduces new challenges in on-chip network design to maintain 
the throughput and energy efficiency of RDAs.
Furthermore, managing and using RDAs at this scale also require new strategies in mapping, 
memory management, and flexible control to fully utilize the compute power of the accelerator. 

In this work, we focus on two aspects of the software-hardware codesign that impact the usability
and performance of the accelerator. One of the biggest challenges that hinders the adoption of these
accelerators is the low-level declarative configuration interface that requires the programmers to
have detailed knowledge about the underlying microarchitecture implementation and hardware
constraints. To address the programmability challenge, we introduce a compiler stack that provides a high-level
programming interface that efficiently translates imperative control constructs to streaming
dataflow execution with minimum synchronization overhead on an on-chip distributed architecture. The
compiler handles the hardware constraints systematically with resource virtualization. To address
the performance challenges, we present a comprehensive study on the on-chip network design for 
reconfigurable dataflow architectures that sustain performance in a scalable fashion with high energy efficiency.
