\chapter{Related Work} \label{sec:related}

Multiple decades of research have resulted in a rich body of literature, both in CGRAs~\cite{cgraSurvey1, cgraSurvey2} and on-chip networks~\cite{ocn-synthesis}. We discuss relevant prior work under the following categories:

\section{Traditional CGRAs and their Compilers}
Emerging around the 1990s and rapidly developed in 2000s, CGRAs were first introduced as near-ASIC efficient accelerators with
post-fabrication reconfigurability that provide a high-speed 
DSP datapath in a very long instruction word (VLIW) design~\cite{cgrasurvey,cgra1,cgra2,adres,trips,pactxpp,dyser,ti}.
Most of the early CGRAs are small reconfigurable fabrics tightly integrated with a 
processor~\cite{dyser, adres,morphosys,piperanch}. 
The processor frequently invokes and reconfigures the CGRA to accelerate compute-intensive regions of the program.
For example, DySER communicates data and enables memory accesses through instruction registers set and
received by a processor~\cite{dyser}.
ADRES uses a shared register file to communicate with a VLIW core~\cite{adres}.
MorphoSys extends the RISC ISA to configure the accelerator and initiate memory transfer
to the accelerator's on-chip buffer~\cite{morphosys}.
The accelerator, often time, handles a subgraph of a basic block at a time, leaving the control
handling to a CPU~\cite{dyser, adres, piperanch}. 
PipeRanch~\cite{piperanch}, for example, require a straight-line single assignment program, 
achieved by fully unrolling loops and inlining functions.
This strategy, however, does not work with applications requiring data-dependent control flow and irregular memory
accesses, such as sparse applications, and dense applications with intensive computation that are
too expensive to fully unroll.

The co-processor model and low-level ISAs in these CGRAs introduce challenges in the programmability
of the hardware.
\cite{dyser} identifies a series of manually performed transformations required to efficiently convert the input program to the expected program graph of a DySER array.
Prior works also have looked into using integer linear programming (ILP) to address constraints in traditional CGRAs. 
\cite{nowatzki} uses ILP to optimize the scheduling, placement, and routing of a dataflow graph within a basic block on the DySER fabric.

Nonetheless, this level of CGRA-processor integration undermines the processor performance due to frequent accelerator
interruptions. 
Furthermore, it is hard to scale in compute and memory density demanded by today's data-analytic
workloads.
Plasticine is a recently proposed large-scale CGRA 
designed as an independent accelerator with dedicated accelerator DRAM~\cite{plasticine}.
The host communicates to the accelerator via a PCIe bus, similar to an FPGA and a GPU.
With the flexibility of a CGRA, Plasticine delivers comparable compute and memory density to
a fixed-function ML accelerator.

At high-level, Plasticine resembles a DySER array replacing the function units with a compute or
memory tile, each supplying a compute density comparable to the entire DySER array.
As a large reconfigurable accelerator, Plasticine takes a longer time to reconfigure, around 10s
of $\mu s$ as supposed to 10s of cycles in traditional CGRAs~\cite{dyser}.
As a result, the expected reconfiguration window for Plasticine is around milliseconds or even
seconds.
To keep the accelerator busy for such a long period of time, Plasticine must support more flexible
control flow in the program to minimize host intervention that compromises the utilization of the hardware.
The consideration of flexible control supports with scalable performance and memory
bandwidth is what differentiates \name's approach from conventional CGRAs' mapping strategy.
While Chlorophyll targeting a GreenArrays 144 chip also support branches, their compute tile is a processor that can execute instructions unlike the SIMD pipeline in Plasticine~\cite{synaid}. 
Chlorophyll focuses on data partitioning across distributed scratchpads. However,
they do not explore pipeline parallelism across compute tiles.
Instead of accelerating a single basic block at a time in traditional CGRAs, 
\name pipelines the entire program with nested control flow onto Plasticine.
By exploring multiple-degrees of concurrency, including instruction-level, data-level, and kernel-level
pipelining and parallelism, \name achieves a good utilization on kernels with both massive and fine-grained
basic blocks.
%The difference in scale determines a completely different mapping strategy that \name exploits compared
%to traditional compiler works on CGRAs.

Wave DPU is another large-scale RDA developed commercially that targets deep learning applications.
It uses a dataflow execution model with a globally asynchronous and locally synchronous (GALS) scheme
that ensures a highly scalable architecture~\cite{wavecomputing}.
With three levels of hierarchy, Wave DPU includes a mesh of CGRA clusters partitioned into compute
machines that provide multiple memory and data I/O ports to an SoC AXI4 NOC; each cluster
further contains a grid of accumulation PEs and arithmetic units.
Like Plasticine, Wave DPU is also an independent accelerator that does not require a CPU to facilitate
execution.
Wave claims to be able to execute any language that can be compiled to an LLVM IR.
From the description of the hardware, it is unclear whether Wave can handle data-dependent control flow and
irregular memory accesses outside of the deep learning domain.

%\subsection{Streaming Dataflow IRs}
%Many recent work in high-level DSL uses a dataflow oriented programming abstraction.
%Example includes ML frameworks like TensorFlow~\cite{tensorflow} that uses a dataflow graph to
%describe the network architecture.
%These dataflow graphs, however, cannot be directly mapped to a dataflow architecture, due to the
%mismatching in their node definitions.
%A node in TensorFlow data graph corresponds to a kernel, which itself is 
%one or multiple invocations of accelerator kernel that contains a sub dataflow graph.
%\name provides an imperative programming front-end for Plasticine to implement these kernels in a
%high-level abstraction that enables cross-kernel optimizations.

%StreamIt is a DSL for streaming applications targeting the RAW architecture,
%a microprocessor array exposing scalable ISAs that maps instructions across
%processors.
%StreamIt have explicit streaming interface across processing pipelines, similar to the output of
%\name's imperative to streaming transformation.
%Unlike Plasticine, the compute engine for RAW is a processor core that can execute arbitrary
%controlflow.

%Although many works claim to emit efficient and information-rich dataflow IRs for the downstream compilers, 
%very few of them can capture the high-level parallel patterns and implementation details that are critical 
%to RDA mappings. 
%For example, TensorFlow \cite{tensorflow} emits dataflow IR composed of tensor operations. 
%However, its IR lacks information on the parallel patterns within these operations. 
%In contrast, most of 
%the streaming languages \cite{streamit, synaid, maxj} are not able to extract nested loop-level parallelism 
%from modern data-intensive applications. 
%For example, StreamIt \cite{streamit}, a language tailored for streaming computing, also adopts distributed 
%control as in \name{}. 
%However, it lacks the necessary language features to describe deeply and irregularly nested loops that are 
%common in modern data-intensive applications.

%\paragraph{Hardware Architectures}
%Spatial reconfigurable accelerators (\eg, Dyser~\cite{dyser} and Tartan~\cite{tartan}) have only one-level of hierarchy.
%Hence, such accelerators' performance can be bottlenecked by their limited interconnect bandwidth and power budget.
%Sparse Processing Unit (SPU)~\cite{sparseaccel} can sustain higher interconnect bandwidth by introducing on-chip hierarchy; however, it lacks support for polyhedral memory banking~\cite{poly_cong}, a pivotal optimization to achieve massive parallel accesses to on-chip memory. Plasticine~\cite{plasticine} provides us with the desired architecture features; however, its compiler lacks the necessary components to support efficient streaming execution. Given that Plasticine resembles many key features of the RDA model, we target Plasticine with \name{}.

%\paragraph{Machine Learning Compilers}
%\begin{outline}
%\1 TensorFlow
%\1 Theano: A Python framework for fast computation of mathematical expressions.
%\1 Cognitive computing programming paradigm: a corelet language for composing networks of neurosynaptic
%cores. I
%\cite{onnc}
%\end{outline}

%Library approach
%\begin{outline}
%\1 Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems
%\1 Anintroductiontocomputational networks and the computational network toolkit
%\1 Neural Network Transformation and Co-design under Neuromorphic Hardware Constraints.
%\1 Caffe: Convolu- tional architecture for fast feature embedding.
%\1 Cambricon: An instruction set architecture for neu- ral networks.
%\1 Tangram: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators
%\end{outline}

%Kernel-specific loop optimization
%\begin{outline}
%\1 Tangram: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators
%\end{outline}

%\paragraph{ML Accelerators}
%\begin{outline}
%%% Dataflow accelerators
%\1 A runtime reconfigurable dataflow processor for vision
%\1 Tangram: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators
%\1 %% domain-specific loop fashion interchange
%\1 Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip. from
%IBM
%\1 Memristive Boltzmann machine: A hardware accelerator for combinatorial optimization and deep
%learning. 
%\1 Scalable hierarchical network-on-chip architecture for spiking neural network hardware
%implementations. 
%\1 Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning.
%\1 Pudiannao: A polyvalent machine learning accelerator. In ACM SIGARCH Computer Architecture News, Vol.
%43. ACM, 369–381.
%\1 Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks. I
%\1 EIE: efficient inference engine on compressed deep neural network.
%\1 Design and evolution of modular neural network architectures
%\1 In-Datacenter Performance Analysis of a Tensor Processing Unit. 
%\1 RENO: A high- efficient reconfigurable neuromorphic computing accelerator design.
%\1 Convolution engine: balancing efficiency \& flexibility in specialized computing.
%\1 Minerva: Enabling low-power, highly accurate deep neural network accelerators. 
%\1 DNPU: An 8.1TOPS/W Reconfigurable CNN-RNN Processor for General Purpose Deep Neural Networks. 
%\1 C-Brain: A deep learning accelerator that tames the diversity of CNNs through adaptive data-level
%\1 parallelization. 
%\end{outline}

%\subsection{Spatial Compilers}
%Most previous works \cite{nowatzki, spatial-computation} only consider allocating resources at the same level. 
%\name{} takes a more general assumption by co-allocating resources at multiple levels of an accelerator's hierarchy.

%The Plasticine compiler~\cite{plasticine} is similar to \name{} that it also uses a token-based control protocol.
%However, it performs worse than \name{} due to the following reasons.
%First, the Plasticine compiler allocates VBs for every level of Spatial's (a high-level language) control hierarchy. 
%The communication between parent and child controllers lead to both communication hotspots around the parent, and bubbles before entering a steady-state of the loop iterations. 
%Second, the Plasticine compiler assigns a single memory PB for each logical memory in the Spatial program. 
%Hence, it could not handle the case where a logical memory exceeds the capacity or bank limits of the physical PBs.
%Third, the Plasticine compiler only supports polyhedral memory partitioning at the first dimension of the on-chip memory. 
%Hence, its applicability to data-intensive applications with high-dimension tensor algebra is questionable.
%Last, compared to \name{}'s separate allocation and assignment phases described in
%\Cref{sec:control} and \Cref{sec:resalloc},
%the Plasticine compiler allocates one VB for a specific type of PB and underutilizes resources within PBs.

\section{On-Chip Networks}

\subsection{Tiled Processor Interconnects} Architectures such as Raw~\cite{raw} and Tile~\cite{tile} use scalar operand networks~\cite{son}, which combine static and dynamic networks. Raw has one static and two dynamics interconnects: the static interconnect is used to route normal operand traffic, one dynamic network is used to route runtime-dependent values which could not be routed on the static network, and the second dynamic network is used for cache misses and other exceptions. Deadlock avoidance is guaranteed only in the second dynamic network, which is used to recover from deadlocks in the first dynamic network. However, as described in Section~\ref{sec:network}, wider buses, and larger flit sizes create scalability issues with two dynamic networks, including higher area and power. In addition, our static VC allocation scheme ensures deadlock freedom in our single dynamic network, obviating the need for deadlock recovery.
The dynamic Raw network also does not preserve operand ordering, requiring an operand reordering mechanism at every tile.

TRIPS~\cite{trips} is a tiled dataflow architecture with dynamic execution. TRIPS does not have a static interconnect but contains two dynamic networks~\cite{trips-network}: an operand network to route operands between tiles, and an on-chip network to communicate with cache banks. Wavescalar~\cite{wavescalar} is another tiled dataflow architecture with four levels of hierarchy, connected by dynamic interconnects that vary in topology and bandwidth at each level. The Polymorphic Pipeline Array~\cite{ppa} is a tiled architecture built to target mobile multimedia applications. While compute resources are either statically or dynamically provisioned via hardware virtualization support, communication uses a dynamic scalar operand network.

\subsection{CGRA Interconnects}
Many previously proposed CGRAs use a word-level static interconnect, which has better compute density than bit-based routing~\cite{bus-fpga}. CGRAs such as HRL~\cite{hrl}, DySER~\cite{dyser}, and Elastic CGRAs~\cite{elasticCGRAs} commonly employ two static interconnects: a word-level interconnect to route data and a bit-level interconnect to route control signals.
Several works have also proposed a statically scheduled interconnect~\cite{van2009static, dimitroulakos2006exploring, wave} using a modulo schedule. While this approach is effective for inner loops with predictable latencies and fixed initiation intervals, variable latency operations and hierarchical loop nests add scheduling complexity that prevents a single modulo schedule. HyCube~\cite{hycube} has a similar statically scheduled network, with the ability to bypass intermediate switches in the same cycle. This allows operands to travel multiple hops in a single cycle, but creates long wires and combinational paths and adversely affects the clock period and scalability.
%However, applications with a hierarchical nesting of loops provide two poor options to arrive at a single modulo schedule: the II can be extended to cover the entirety of the innermost loop, or the outer loop can have resources reserved in the inner loop schedule.
%The first option is frequently not realistic because scheduled hardware has a hard cap on the II, and loops can be of arbitrary length.
%If outer loops have resources reserved in the inner loop schedule, then the schedule will become congested with reserved, but infrequently used resources.
\subsection{Design Space Studies} Several prior studies focus on tradeoffs with various network topologies, but do not characterize or quantify the role of dynamism in interconnects.
The Raw design space study~\cite{dse-raw} uses an analytical model for applications as well as architectural resources to perform a sensitivity analysis of compute and memory resources focused on area, power, and performance, without varying the interconnect. The ADRES design space study~\cite{dse-adres} focuses on area and energy tradeoffs with different network topologies with the ADRES~\cite{adres} architecture, where all topologies use a fully static interconnect. KressArray Xplorer~\cite{dse-kressarray} similarly explores topology tradeoffs with the KressArray~\cite{kress} architecture. Other studies explore topologies for mesh-based CGRAs~\cite{dse-date} and more general CGRAs supporting resource sharing~\cite{dse-tvlsi}. Other tools like Sunmap~\cite{sunmap} allow end-users to construct and explore various topologies.

\subsection{Compiler-Driven NoCs}
Other prior works have used compiler techniques to optimize various facets of NoCs.
Some studies have explored statically allocating virtual channels~\cite{staticVC-isca, staticVC-nocs} to multiple concurrent flows to mitigate head-of-line blocking. These studies propose an approach to derive deadlock-free allocations based on the turn model~\cite{turnModel}. While our approach also statically allocates VCs, our method to guarantee deadlock freedom differs from the aforementioned study as it does not rely on the turn model.
Ozturk et al. \cite{ozturk2010compiler} propose a scheme to increase the reliability of NoCs for chip multiprocessors by sending packets over multiple links.
Their approach uses integer linear programming to balance the total number of links activated (an energy-based metric) against the amount of packet duplication (reliability).
Ababei et al. \cite{ababei2011energy} use a static placement algorithm and an estimate of reliability to attempt to guide placement decisions for NoCs.
Kapre et al. \cite{kapre2011noc} develop a workflow to map applications to CGRAs using several transformations, including efficient multicast routing and node splitting, but do not consider optimizations such as non-minimal routing.

%Our approach provides an extension of these techniques by using a ``closed-loop'' iterative process.
%Instead of doing each step (placement, routing, deadlock avoidance) sequentially, we are able to use heuristics to quantify the impact that each step has on the badness of the overall design.
%For example, this allows us to fix routing pathologies by re-placement because we can identify the specific placement decisions that led to the poor set of routes.



% \gist{
% CGRA: \\
% \cite{tartan} \\
% Imperative to Spatial Architectures: \\
% \cite{synaid}: 
% \begin{outline}
% \1 Target green array. Tiles of stack based 18-bit processors. Small local memory per
% core.
% \1 Small benchmarks: trigonometric, FFT, bithack, etc..
% \end{outline}
% \cite{zaidi}
% His thesis:\\
% \url{https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-870.pdf} \\
% Targeting FPGA. We probably don't want to cite this guy.

% Partitioning: \\
% \cite{nowatzki} - Partitioning data flow graph using ISL \\

% Summary:
% Looks like we are not the first one trying to map imperative language to spatial architecture. But there are still major differences.
% Some of the work is mapping to FPGAs, similar to high-level synthesis tools.
% Many of the other works like \cite{zaidi} only focus on the data-flow graph and do not handle memory accesses. Although we both trying to support imperative language on the spatial accelerator, our focus is never to accelerate control heavy code on the accelerator, but rather to target data-intensive application with more flexibility.

% \cite{synaid} is actually very relevant but the architecture is at a much smaller scale. It does have distributed on-chip memory but their memory is a stack used to store programs for processors. None of the prior work has considered using distributed memory to compose logically single memory with strong-consistency and coherence. Another major difference is that these architecture does not have a global network and DRAM that introduces variable latency. 
% So this changes how the application is mapped onto the accelerator. Our approach is a completely distributed streaming approach while they tend to use a statically scheduled approach.
% Finally the applications are very different. Most of their applications are very small kernels or image/audio encoding/decoding. Our benchmarks have a mix of full ML models + graph + others
% }

% Maybe related \cite{sparseaccel}

% Triggered Instruction \cite{ti}
% Tartan\cite{tartan}

% \gist{
%     \begin{itemize}
%         \item Plasticine compiler: \cite{plasticine}
%         \item Tartan: \cite{tartan} Single op partitions w/ handshaking, direct template translation.
%         \item Zaidi Thesis: \cite{zaidi} Conversion to bluespec, also uses token-based control. No partitioning / merging.
%         \item Partitioning: \cite{nowatzki} Solver-based partitioning, very small arrays (4x4 dyser, etc.), with statically known delays and single-op partitions. Does not handle merging.
%         \item SparseAccel: \cite{sparseaccel}\ Architecture paper, basically nothing about compilers.  (NR)
%         \item streamit for RAW: \cite{streamit} No memory consistency due to pure stream structure, fine-grained arch. Mesh of processors.
%         \item Triggered Instructions \cite{ti} Fine-grained processor architecture, with caches. (Micro paper: manual mapping, assembly, NR)
%         \item Chlorophyll: \cite{synaid} Maps subset of C to GreenArrays (small spatial stack-processor architecture). Distinct Non-distributed arrays vs distributed arrays, optional partition annotations, static for loops (unrestricted while), uses Rosette backend solver (branch and bound, SA, Ant, Tabu Search, picked Simulated Annealing). The actual generation is synthesis based.
%         \item Spatial computation \cite{spatial-computation} Maps ANSI C to Verilog, handshake + token-based execution with speculative execution. Uses a crossbar network for memory (although we assume pre-banked memory). Fine-grained control uses Pegasus for memory dependence graph and tokens for synchronization. Does not address partitioning/merging.
%     \end{itemize}
% }
