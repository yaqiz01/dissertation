\section{Imperative to Streaming Transformation}
\label{sec:control}

%In a na\"ive approach, we can map each controller in the hierarchy into a VB (\Cref{fig:centralctrl}).
%This strategy suffers from expensive network round-trip delays between the parent and child controllers.
%If the parent controller is an unrolled loop, the parent needs to synchronize with all child controllers, which creates an undesired communication hot spot.
%\Cref{fig:centralctrl}(a) shows an example where synchronization {\em just} between parent and child controllers can produce an incorrect result due to unpredictable network latency.

%The alternative approach explores a different way to execute the expected control schedule correctly. 
%The minimum required synchronization to produce the correct result is to ensure that the computations access the intermediate results in a consistent matter as if the control schedule is strictly enforced. 
%This can be achieved via p2p synchronizations \emph{only} between computations that access a particular shared memory.
%The execution order of computations that access different memories does not need to be enforced, as they do not impact the program outcome.
%Therefore, as long as the compututation is executed with the expected number of iterations and the memories are updated consistently, there is no need for any extra synchronization.
%Next, we walk through how \name{} achieves this in more concrete detail.

\subsection{Loop Division (Ready)}
Between the front-end and the back-end abstractions of \name, an obvious gap is that 
the imperative front-end language can contain arbitrarily nested control hierarchy, whereas
the hardware compute engine can only execute operations that are control-free.
To address this issue, we introduce a new type of loop transformation---loop division---for streaming reconfigurable
accelerators.
Similar to loop fission, loop division breaks a single loop into multiple loops.
The difference is that loop fission generates a sequence of sequentially executed loops, whereas
loop division generates loops executing \emph{concurrently}.
Additionally, loop fission materializes the intermediate results across fissioned loops into arrays,
while loop division use queue to communicate across loops.
Each loop generated from loop division can only execute if all of their input queues are not empty.
\Cref{fig:loopexp1} gives an example of a loop fusion vs. loop division.

\begin{figure*}
\centering
\begin{subfigure}[b]{0.28\textwidth}
\inputminted{python}{code/loopexp1.py}
\caption{Input program}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.31\textwidth}
\inputminted{python}{code/loopexp1fission.py}
\caption{Loop Fission}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\inputminted{python}{code/loopexp1division.py}
\caption{Loop Division}
\end{subfigure}
\caption[Example of loop fission vs. loop division]{
  (b) and (c) shows the output of loop fission and loop division of the input program (a), respectively.
  In (b), the first loop is executed entirely before executing the second loop. The intermediate
  result \texttt{tmp} is materialized into an array with the same size as the loop range.
  In (b), the two loops can execute concurrently. The intermediate result is materialized into a
  queue. For each iteration, a loop can execute only if all of its queues are non-empty.
  The second loop can execute as soon as \texttt{tmp} receives the first element.
}
\label{fig:loopexp1}
\end{figure*}

When executing loop division on a single-threaded CPU, the CPU must context switching between the
concurrent loops
and executing the one with cleared input dependencies.
Like loop fission, loop division is likely worsening the performance on a processor architecture, as
the worst-case memory footprint of the intermediate result \texttt{tmp} increases from $O(1)$ to $O(N)$.
On RDAs, the divided loops are executing
concurrently in a streaming pipelined fashion. The size of the \texttt{tmp} can be limit to a small fixed
size, efficiently implemented with a hardware FIFO. 
Although loop transformations are generally optimizations on CPUs,
loop division is a required transformation to converts an infeasible program to a feasible one for Plasticine.

Loop fission is not always safe, as it may alter the execution order of the program.
Loop division, on the other hand, does not change the underlying data-dependency and is always safe.
To achieve this, loop division needs to introduce additional dummy data dependencies across divided
loops to enforce the correct execution order.
\Cref{fig:loopexp2} gives an example of an invalid loop fission and a correct loop division.
\Cref{fig:sync} gives more detail on how \name automatically generates the dummy data-dependencies.

\begin{figure*}
\centering
\begin{subfigure}[b]{0.28\textwidth}
\inputminted{python}{code/loopexp2.py}
\caption{Input program}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\inputminted{python}{code/loopexp2fission.py}
\caption{Invalid Loop Fission}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.31\textwidth}
\inputminted{python}{code/loopexp2division.py}
\caption{Loop Division}
\end{subfigure}
\caption[Example of an illegal loop fission and a legal loop division]{
Example of an illegal loop fission and a legal loop division
}
\label{fig:loopexp2}
\end{figure*}

\subsection{Virtual Context Allocation (Ready)} 

\begin{figure*}
\centering
\begin{subfigure}[b]{0.4\textwidth}
\inputminted{python}{code/spatialeg.py}
\caption{Pseudo input example}
\label{fig:contexteg}
\inputminted{python}{code/contextalloc.py}
%\missingfigure[figwidth=1\textwidth]{Spatial IR}
\caption{Initial context allocation}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.5\textwidth}
\inputminted{python}{code/contextsplit.py}
\caption{Request and response division}
\end{subfigure} \\
\vspace{0.2cm}
\begin{subfigure}[b]{0.23\textwidth}
%\includegraphics[width=1\textwidth]{figs/dep.pdf}
\includegraphics[width=1\textwidth]{figs/ctxdag.pdf}
\caption{Context Graph}
\end{subfigure}
\begin{subfigure}[b]{0.76\textwidth}
\includegraphics[width=1\textwidth]{figs/plasticinetiming.pdf}
\caption{Timing on Plasticine}
\end{subfigure}
\caption[Context allocation]{
  Lowering example.
  %Same example as \Cref{fig:spatialegpar} without outer loop
  %unrolling factor equals to 1.
  %(a) has two basic blocks within the inner most controllers \texttt{B} and \texttt{C}.
  \name allocates one context per basic block for \texttt{B} and \texttt{C}, shown in (b). Outer controller \texttt{A} is
  duplicated in both \texttt{ctxB} and \texttt{ctxC}.
  (c) \name seperates out requestion generator \texttt{rqstR1} from \texttt{ctxB} for \texttt{R1} 
  and response receiver from \texttt{ctxC} \texttt{respW1} for \texttt{W1}.
  The resulting dataflow graph is shown in (d). 
  To enforce the forward data-dependency between \texttt{W1} and \texttt{R1}, 
  \name allocates a forward token between \texttt{W1}'s response receiver \texttt{respW1} and
  \texttt{R1}'s request generator \texttt{rqstR1};
  to enforce the loop-carried WAR dependency between \texttt{R1} and \texttt{W1}, \name allocates a
  backward token (\texttt{credit}) between \texttt{R1}'s response receiver \texttt{ctxC} and 
  \texttt{W1}'s request generator \texttt{ctxB}. 
  The backward credit is initialized with two elements because \texttt{mem} is double-buffered.
  On the writer side, a forward \texttt{token} is a
  produced and a backward \texttt{credit} is consumed every \texttt{B} iterations; on the reader
  side, a forward \texttt{token} is consumed and a backward credit is produced every \texttt{C}
  iterations. The resulting timing of the execution is shown in (e).
}
\label{fig:contextalloc}
\end{figure*}

\begin{figure*}
\centering
\begin{subfigure}[b]{0.5\textwidth}
  \centering
\includegraphics[width=0.6\textwidth]{figs/densespecial.pdf}
\caption{Context dataflow graph}
\includegraphics[width=0.7\textwidth]{figs/denseaddr.pdf}
\caption{Optimization for long address calculation}
\label{fig:densespecial}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
  \centering
\inputminted{python}{code/densectx.py}
\caption{Dense specialization}
\end{subfigure} \\
\vspace{0.2cm}
\begin{subfigure}[b]{1\textwidth}
  \centering
\includegraphics[width=0.85\textwidth]{figs/densetiming.pdf}
\caption{Timing on Plasticine for dense on-chip memory}
\end{subfigure}
\caption[Dense specialization]{
  Specialization for dense memory lowering.
  The same example as \Cref{fig:contextalloc} (a) if the memory is a dense scratchpad.
  (a) and (b) shows the allocated contexts.
  Context \emph{rqstW1} and \emph{rqstR1} are allocated \emph{in the same} VU as the memory. The latency of 
  the forward token and the backward credit take only one cycle, which are not shown in (d).
  The latency to compute read and write addresses are also not shown.
  For complex address calculation, \name split the address computations to separate contexts shown
  in (c). The addresses can be precomputed without waiting for \emph{wdata} for $rqstW1$ and 
  \emph{token} for $rqstR1$.
}
\label{fig:densespecial}
\end{figure*}

At high-level, \name executes the entire program in a pipelined fashion, where each basic block
in the program is a pipeline stage.
As a start, \name allocates virtual memories to hold the data-structures, and 
contexts to execute basic blocks within the innermost controllers. 
A basic block maps naturally to a context, as instructions within a basic block are control-free. 
\name makes a copy of all controllers enclosing the basic block in the corresponding context;
these controllers are later converted to counters and control configurations supported by the
hardware. 
Effectively, \name performs loop division such that all basic blocks are perfectly nested.
Contexts are concurrent workers producing and consuming the intermediate data-structures across basic
blocks.

With these controllers, each context can repeatedly execute their basic blocks for an expected number iterations. 
However, data-structures written and read by different contexts are still accessed in random order.
Next, for each data-structure, \name examines all contexts accessing the memory,
allocating synchronization tokens across these contexts.
The control token can be viewed as an access grant to the intermediate memory passing between 
the pipelined contexts; the token guarantees the access order of the concurrent contexts is consistent to
the expected order of an imperative program.
The insight is that \emph{as long as all memories are accessed with a program-consistent
order, the final result is identical to a sequentially executed program}.
This way, \name introduces minimum p2p synchronizations among small groups of contexts; contexts
accessing different data-structures are naturally parallelized without impacting the final output.
\Cref{fig:contextalloc} (b) shows an example of the context allocation. 
\Cref{sec:controlalloc} explains how \name allocates the synchronization tokens based on the control
hierarchy of the imperative program.

Unlike traditional out-of-order execution, where both static scheduling in software and dynamic
scheduling in hardware search for independent instructions to execute concurrently, 
\name starts with executing \emph{all} basic blocks of a program concurrently,
introducing minimum synchronizations wherever necessary.
Unsurprisingly, a control-heavy program with lots of basic blocks can easily run out of PUs.
Nonetheless, the intended workload for RDAs are data-analytical programs, which have relatively simple
control structures with abundant of data-level parallelism (i.e., most basic blocks having a high repetition
count).
The control structures, however, might not be simple enough that a SIMT architecture, designed for
massive embarrassingly parallel workloads, can still be heavily underutilized.
To map a large program on Plasticine, the program graph must be sliced into multiple chunks.
A single chunk is executed in-space on-chip, exploring parallelism and pipelining;
different chunks are executed in-time by reconfiguring the accelerator.

\subsection{Control Allocation (WIP)} \label{sec:controlalloc}
\label{sec:sync}
\name uses control tokens to enforce the memory access order in an imperative program on a dataflow
architecture. This section answers three essential questions for this transformation: \emph{how} to
enforce the order of two distributed pipelined memory accesses; \emph{where} should the control
token be inserted; and \emph{when} should token be sent at runtime.
%Starting with all contexts execute in parallel, \name introduces \term{control token}s across contexts
%to serialize their execution order based on the program order.
%This control token is no different from a regular data-dependency and can be viewed as 
%an access grant to the shared memory across contexts. 
%By controlling {\em where}, {\em how}, and {\em when} to pass the token, \name is able to maintain a consistent update ordering between the pipelined and parallelized contexts that access the shared memory.

%We refer to a memory access appeared in the input IR as a \emph{declared access}, as supposed to
%memory accesses executed at runtime.
%\texttt{W1} and \texttt{R1} are examples of two declared accesses in \Cref{fig:contexteg}
%In the rest of this section, we will walk through how \name allocates control tokens
%to maintain sequential consistency on Plasticine.

\paragraph{How.} 
The first question is how do we enforce the access order of a memory from two remotely distributed contexts, given the
global network and the memory itself can introduce unpredictable latency.
The streaming memory interface provides a response packet per request packet; the response is
the data for read and an acknowledgment for write.
To eliminate the round-trip latency, \name further divides a memory access declared in the program into 
a requesting and a receiving context, as shown in \Cref{fig:contextalloc} (c). 
For a read access declared in the input IR, \name maps the address generation in a separate contexts, streaming requests to
the memory and streaming the data to the receiver context.
For a write access, \name allocates a separate context to accumulate acknowledgments.

%We refer to a memory access appeared in the input IR as a \emph{declared access}, as supposed to
%memory accesses executed at runtime.
%\texttt{W1} and \texttt{R1} are examples of two declared accesses in \Cref{fig:contexteg}
To order an access $A$ before an access $B$,
\name allocates a token between the receiving context of access $A$ ($resp_A$) and the requesting context
of access $B$ ($rqst_B$).
This is essential to ensure the memory effect of access $A$ is visible to access $B$.
For any two accesses enclosed by a loop, there could be a loop-carried dependency (LCD) from the
the access appeared later to the access appeared earlier in the program order.
When inserting token for LCD, the receiver's token buffer is initialized with at least one token,
enabling the earlier access for the first iteration of the loop. \Cref{fig:contextalloc} (c) shows the
forward and the backward token to enforce two accesses under a loop. The backward tokens for LCD are
always initialized with at least one element to break the cyclic dependency. Borrowing the flow-control
terminology\cite{credit}, we refer the backward token as a credit.
If the memory is multi-buffered, the credit is initialized buffer depth $D$ number of credit, 
enabling the first access to operate for $D$ iterations of the loop before back-pressured.

The requesting and receiving contexts pair are general schemes we we use on any type of memory with 
multiple cycle access latencies, such as the off-chip DRAM.
In the common cases, we can simplify the synchronization logic for certain on-chip memories.

\subparagraph{Specialization for dense on-chip scratchpad}
Spatial can statically analyze the dense access pattern of a on-chip array and partitions the memory
to avoid bank conflicts. These memory has a guaranteed single cycle access latency, 
which simplifies the synchronization.
\name allocates $rqst_A$ and $rqst_B$ contexts \emph{in the same} VB as the accessed memory.
Instead of synchronizing between $resp_A$ and $rqst_B$, \name sends the forward token and the
backward credit between $rqst_A$ and $rqst_B$,
eliminating the need for a receiving context for the write access.
Because memory requests are served in a single cycle, access $A$'s requests would be visible to
access $B$ by the time $rqst_B$ receives the token. Hence, write acknowledgment is no longer needed.
\Cref{fig:densespecial} shows the resulting contexts if \emph{mem} in \Cref{fig:contexteg} is a dense
on-chip array.

\subparagraph{Specialization for non-indexable memory}
For most non-indexable memories like scalar variables and queues in the program, \name directly maps
them to the input buffer of the receiver context if the memory has a single writer and a single
reader.
Non-indexable memory with multiple readers and writers also need synchronization token across
access contexts.
%We perform another specialization on non-indexable memories (registers or FIFOs), whose all accesses have no explicit read enables.
%Instead of treating them as shared memories, \name{} duplicates and maps them to local input buffers in all receivers, no longer requiring tokens.
%The sender actor pushes to the network when the token is supposed to be sent, and the receiver dequeues one element from the input buffer when the token is supposed to be consumed.
%This dramatically reduces the synchronization complexity of non-indexable memory in the common case.


\begin{table*}
  \centering
\begin{tabular}{lccc}
  \toprule
 Data structure & Memory type \\ \midrule
  array (fit on-chip) & SRAM \\
  array (not fit on-chip) & DRAM \\
  scalar variable & register \\
  queue & FIFO \\
 \bottomrule
\end{tabular}
\caption[Mapping between data-structure to hardware memories]{
  Mapping between user declared data-structure to underlying hardware memories. 
  Programmers explicitly specify the desired hardware type inside Spatial. 
  In other languages, this table specifies a mapping between software data-structures 
  and hardware memory types on Plasticine.
}
\label{tab:memtype}
\end{table*}

\paragraph{Where.}
Although virtually all accesses of a memory depends on each other, \name does not needs to insert
token to enforce dependency between each pair of accesses. The second question figures out the
minimum number of accesses \name needs to order to provide sequential consistency.
\name synchronizes two accesses if they potentially interfere.
Whether two declared accesses interfere depends on 
\begin{outline}
  \1 the type of accesses (read vs. write)
  \1 the type of the memory (e.g. SRAM, DRAM)
  \1 and location of the declared accesses in the control hierarchy.
\end{outline}

\begin{table*}
  \centering
\begin{tabular}{lcccc}
  \toprule
  Memory type             & DRAM   & SRAM   & FIFO   & Register \\ \midrule
  read-after-read (RAR)   & \xmark & \cmark & \cmark & \cmark \\
  read-after-write (RAW)  & \cmark & \cmark & \cmark & \cmark \\
  write-after-read (WAR)  & \cmark & \cmark & \cmark & \cmark \\
  write-after-write (WAW) & \cmark & \cmark & \cmark & \cmark \\
 \bottomrule
\end{tabular}
\caption[Interferance Table]{
  Interference table for whether two accesses interfere for each memory type.
}
\label{tab:interferetab}
\end{table*}

\Cref{tab:memtype} lists hardware memories available on reconfigurable architectures and software data-structures providing similar program semantics.
The type of the memory matters because they have different programming interface on the hardware.
For instance, two DRAM read accesses do not interfere because the DRAM interface permits
multiple concurrent access streams through multiple DAGs\footnote{All DAGs can access the full DRAM
address space}. 
%\gist{mension port virtualization}
Therefore, from the programmer's perspective, users do not need
to serialize the two contexts reading the same DRAM address. 
SRAMs, on the other hand, have a single read and write port. Programmers must guarantee that a PMU receives read requests from a single context
at any point in time for correctness.
\Cref{tab:interferetab} shows the interference relation between different types of memory across
accesses.

\begin{figure*}
\centering

\begin{subfigure}[b]{0.5\textwidth}
\inputminted{python}{code/dep1.py}
\caption{Pseudo example}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
  \centering
\includegraphics[width=0.8\textwidth]{figs/dep1.pdf}
\caption{Dependency Graph}
\end{subfigure}
\\
\begin{subfigure}[b]{0.5\textwidth}
\inputminted{python}{code/dep3.py}
\caption{Pseudo example}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
  \centering
\includegraphics[width=0.8\textwidth]{figs/dep3.pdf}
\caption{Dependency Graph}
\end{subfigure}

\caption[Dep Graph]{
  Dep Graph (WIP)
}
\label{fig:depexp}
\end{figure*}

For every declared access, \name{} checks on other accesses appeared earlier in the program order
for a possible forward dependency, and later in the program order for a possible loop-carried dependency (LCD). 
In the example in \Cref{fig:contexteg}, there is a forward data dependency between \texttt{W1} and
\texttt{R1}, and a write-after-read LCD between \texttt{R1} and \texttt{W1}. 

For each memory, \name builds a dependency graph between accesses in the input IR.
\Cref{fig:depexp} gives examples of the dependency graph.

%Enforcing all dependencies in the dependency graph may not be necessary, as enforcing a subset of
%dependencies can be sufficient to enforce the order of the whole graph.
%Therefore, 
To minimize enforced dependencies,
\name performs a transitive reduction (TR) on the graph to keep the minimum number of dependency edges that preserve the same ordering \cite{tr}.
Since TR on a cyclic graph is NP-hard, \name perform TRs on the forward and backward LCD graphs, separately.
Notice, dependencies between accesses touching different buffers of a multi-buffered memory is less rigid than accesses touching the same buffer.
Therefore, we can only remove an edge if all dependencies on the equivalent path have a stronger or equivalent dependency strength than the strength of the removed edge.

%Two declared accesses A and B have no dependency if their least-common ancestor (LCA) controller executes only one of the children at anytime (from a branch), or all children in parallel (from an unrolled loop).
%The LCD exists between B to A, if B occurs later in the program order, and A and B are surrounded by a loop.
%To detect LCD, \name checks if a loop exists among two accesses' LCA controller and LCA's ancestor controllers.

\begin{figure*}
\centering
\includegraphics[width=1.0\textwidth]{figs/synch_mech.pdf}
\caption{
    (a) Access dependency graph.
    (b) Synchronization of two accesses on the same memory.
    (c) Single-cycle special case.
    (d) Actors uses local states of controller hierarchy to determine when to send a token.
}\label{fig:depgraph}\label{fig:token}\label{fig:tokentrick}\label{fig:tokenwhen}
\end{figure*}
%\ms{repeated caption, rather give a single caption.}


\paragraph{When.}
\name{} configures the contexts to generate the token using their local states at runtime.
For FIFOs, the token is generated and consumed every cycle when the producer and receiver contexts are active.
For register, SRAM, and DRAM, the program order expects that the producer and consumer write and inspect the memory once per iteration of their LCA controller, respectively.
Since the producer and receiver both have their outer controllers duplicated in their local state, they have independent views for one iteration of the LCA controller, which is when the controller in their ancestors (that is the immediate child of the LCA controller) is completed (\Cref{fig:tokenwhen}(d)).
The {\em done} signals of these controllers are used to produce and consume the token in contexts, independently.

\subsection{Data-Dependent Control Flow (Ready)}
Using the synchronization discussed in \Cref{sec:sync}, \name can support control constructs that 
typically are not supported on dataflow accelerators, such as branches and while loops.
Most dataflow accelerators do not support control divergence. SIMT architectures, like GPUs,
implement branches with predications and pay the latency penalty of both branch cases.
To enable these flexible control, the control path of the architecture must permit data dependencies.
\Cref{fig:controlarch} details the required changes in the control path to support these features.

\paragraph{Dynamic Loop Range}
\Cref{fig:dynrange} shows an example of loops with data-dependent ranges. 
\name uses a context to compute the loop bounds, which are treated as input dependency to the context 
that maps the loop body.
\begin{figure*}
\centering
\begin{subfigure}[b]{0.4\textwidth}
\inputminted{python}{code/dynrange.py}
\caption{Input program}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.5\textwidth}
\inputminted{python}{code/dynrangectx.py}
\caption{Context data-flow graph}
\end{subfigure}
\caption[Example of dynamic loop range]{
  (a) shows an example program with dynamic loop range. 
  Expressions to generate the loop bounds belongs to a basic block that gets mapped to \texttt{ctx1}.
  \name maps the loop bound as a data-dependnecy to context that maps the inner loop context, and
  configures dequeue signal of \texttt{bound} stream to counter \texttt{B.done}.
}
\label{fig:dynrange}
\end{figure*}

\begin{figure*}
\centering
  \vspace{-1cm}
\begin{subfigure}[b]{0.45\textwidth}
\inputminted{python}{code/branch.py}
\caption{Input program}
  \vspace{0.2cm}
\includegraphics[width=0.8\textwidth]{figs/branchctx.pdf}
\caption{Dataflow graph}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\inputminted{python}{code/branchctx.py}
\caption{Context configuration}
\end{subfigure}
\\
  \vspace{0.2cm}
\begin{subfigure}[b]{\textwidth}
  \centering
\includegraphics[width=0.8\textwidth]{figs/branchtiming.pdf}
\caption{Timing}
\end{subfigure}
\caption[Branching example]{
  An example program with a branch. 
  (d) shows the timing of execution with $A=4$, $D=3$, and $F=2$.
}
\label{fig:branch} 
\end{figure*}

\paragraph{Branch Condition}
\name supports both predictions and actual branches.
Branches within the innermost loops are implemented with predications so that the loop can still be
vectorized across SIMD lanes.
This is very similar to a SIMT architecture, where all lanes execute the \emph{if} clause followed by the \emph{else} clause, masking off the memory access for the disabled lanes.
The total number of stages required is the total operations in both the \emph{if} and the \emph{else} clauses.

For an actual branch in an outer loop,
the branch condition is treated as a data-dependent enable signal for controllers under the branch clauses.
If the controller is disabled, its \emph{done} signal raises to high immediately.
Output tokens depending on the {\em done} signal will be immediately sent out.
This way, the controller under a branch condition does not need to execute if the outer branch
evaluates to false.

\Cref{fig:branch} shows an example with a branch statement.
The input program in (a) writes and reads the memory \emph{mem} on even and odd cycles of
the outer loop \emph{A}, respectively. 
\name maps the three basic blocks in the program in three
contexts, \emph{ctxB}, \emph{ctxD}, and \emph{ctxF}, as shown in (b) and (c). 
For the read and write accesses, \name
allocates a context \emph{respW} to accumulate write acknowledgments and a context \emph{rqstR} 
to generate read requests. 
\emph{ctxB} generates the branch conditions for both the \emph{if} and the \emph{else} clauses.
The conditions are sent as data dependencies to contexts mapping the basic blocks under the branch conditions.

\Cref{fig:branch} (d) shows the timing of execution.
As \emph{ctxB} has no dependencies, \emph{ctxB} computes the conditions for all iterations of
\emph{A} in
pipelined fashion and broadcasts the conditions to the receiver contexts.
The \emph{if} contexts (\emph{ctxD}+\emph{rspdW}) receives a \texttt{true} condition for the first
iteration of \emph{A} ($A_0$), 
executing all iterations of D, and passes a token to the \emph{else} (\emph{rqstR}+\emph{ctxF}) contexts.
Because \emph{mem} is double-buffered, the credit is initialized with two elements in the
receiver's input buffer, enabling the \emph{if} contexts to execute two iterations of outer loop
\emph{A} before
waiting for the \emph{else} contexts. For the second iteration of loop \emph{A} ($A_1$), the \emph{if} contexts
receives a \texttt{false} condition for branch \emph{C}.
Therefore, the enclosing loop controller \emph{D}'s \emph{done} signal is immediately high, sending the
token to \emph{rqstR} context right away.
On the other side, the $A_0$ of the \emph{else} contexts is blocked by the token from
the \emph{if} contexts. 
As soon as the token arrives, the \emph{else} contexts send out the credit immediately,
Next, the \emph{else} contexts check for the second token, 
and executes loop \emph{F} for $A_1$.

Both the \emph{if} and the \emph{else} contexts only execute if their enclosed branch clauses are
evaluated to be true. More interestingly, \Cref{fig:branch} (d) shows a overlapping execution of the
\emph{if} and \emph{else} clauses across iterations of A. 
The hardware for both \emph{if} and \emph{else} clauses are active almost all time.
If the latency of loop \emph{D} and \emph{F} are both $L$, 
the total runtime for $N$ iterations of loop \emph{A} would be on the order of $\frac{N}{2}L+L$
on Plasticine, assuming L and N are large.
\emph{This is almost twice as fast as a traditional coarse-grained pipelining with hierarchical FMS
schedulers}, like Spatial's FPGA back-end, whose runtime is $NL+L$.

\paragraph{Do While Loops}

\begin{figure*}
\centering
  \vspace{-1cm}
\begin{subfigure}[b]{0.45\textwidth}
\inputminted{python}{code/dowhile.py}
\caption{Input program}
  \vspace{0.2cm}
\includegraphics[width=0.8\textwidth]{figs/dowhile.pdf}
\caption{Dataflow graph}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\inputminted{python}{code/dowhilectx.py}
\caption{Context configuration}
\end{subfigure}
\\
  \vspace{0.2cm}
\begin{subfigure}[b]{\textwidth}
  \centering
\includegraphics[width=0.6\textwidth]{figs/dowhiletiming.pdf}
\caption{Timing}
\end{subfigure}
\caption[Do while example]{
  An example program for do while loop. 
  Loop \emph{A} in (a) is a loop running forever, with a conditional \emph{stop} variable.
  The break statements in (c) corresponds to configuring the stop signal of the enclosing controller \emph{A}
  to the output of the stop input buffer. The stop buffer is dequeued every cycle controller \emph{A} is active.
}
\label{fig:dowhile} 
\end{figure*}

The \emph{do while} construct is very useful to express iterative convergence algorithms or handle external
data stream with a last-bit signal that terminates the execution.
A \emph{do while} loop works very similarly to a loop with dynamic range, except the loop has a very long
initialization interval. The earliest starting time for the second iteration of the loop is when the while condition is resolved.
The while condition is a data-dependency to all contexts mapping the basic blocks enclosed by the while loop.
%This while condition is a loop carried dependency, assuming the first value is produced after the first
%iteration.
There is a loop-carried dependency between the producer of the condition within the while loop body
and the loop controller that consumes a value of condition every iteration.
\Cref{fig:dowhile} shows an example with a do-while loop.

%The break statement in (a) translates to a \emph{stop} register associated with the controller of loop
%\emph{A}. 
Loop \emph{A} reads and block \emph{C} writes a value of stop for every iteration of loop
\emph{A}.
\Cref{fig:dowhile} (b) shows the dataflow graph, mapping each basic block to a context.
\emph{ctxC} computes the stop condition, which is used by both \emph{ctxC} and \emph{ctxB}.
There is a loop-carried dependency between the reader of \emph{stop} from loop \emph{A} and the
writer of stop from block \emph{C}. Therefore, the stop streams in (c) are initialized with 
\texttt{False} to enable the execution of the first iteration.
The \emph{accum} variable in (a) has two readers in block \emph{B} and block \emph{D}; one has a
loop-carried dependency with the writer (line 9) and the other has a forward data-dependency (line
11). 
Therefore, the variable is mapped to two copies of \emph{accum}, one for each reader. 
Because the accumulate operation is a single operation ADD, the accumulator gets optimized to the special
accumulate pipeline register \emph{accum1}, 
Without this optimization, \emph{accum1} would be another \texttt{ScalarStream} with an initial
element 0.
The other \emph{accum} becomes a scalar stream \emph{accum2}, written by \emph{ctxB} when loop \emph{A}
is done.
(d) shows the timing of execution. As we can see, the initialization interval across iterations of
A in the steady-state is bounded by the latency to evaluate \emph{stop}, which is the number of
operations in stop expression rounded up to a multiple of 6 stages because the data must
propagate to the end of each SIMD pipeline.

\subsection{Virtual Unit Allocation (Ready)}
After all contexts and shared memory are allocated and synchronized, 
\name moves the remaining floating contexts into their own VUs, indicating the boundary of the physical units.
Certain contexts mentioned in dense SRAM specialization might already belong to the same VU the
memory locates at to ensure the synchronization has a single cycle latency.
In the resource allocation phase, \name might partition the big contexts into multiple VUs, and small contexts into a single VU. When contexts are merged into a single VU, they are still
separate contexts triggered independently.
